{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we will be performing sentiment analysis of Twitter data. We will using the Twitter dataset to access tweets and use natural language processing techniques to classify tweets as positive, negative or neutral. The sentiment analysis model will be trained on a pre-labeled dataset of tweets and will use machine learning algorithms to classify new tweets.\n"
      ],
      "metadata": {
        "id": "4y72ZQhbR6w5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOGISTIC REGRESSION\n"
      ],
      "metadata": {
        "id": "hSkcbWghiIqc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRrHT-27sAaN",
        "outputId": "52053e36-432a-40cf-83d6-31f9aaa0f03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.42      0.23      0.30       258\n",
            "           1       0.94      0.96      0.95      3879\n",
            "           2       0.87      0.92      0.89       820\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.74      0.70      0.71      4957\n",
            "weighted avg       0.90      0.91      0.91      4957\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data.sample(frac=0.8, random_state=42)\n",
        "test_data = data.drop(train_data.index)\n",
        "\n",
        "# Create a CountVectorizer to convert the text data into numerical features\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "train_features = vectorizer.fit_transform(train_data['tweet'])\n",
        "test_features = vectorizer.transform(test_data['tweet'])\n",
        "\n",
        "# Train a logistic regression model on the training data\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(train_features, train_data['class'])\n",
        "\n",
        "# Test the model on the testing data\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "# Calculate precision, recall, and F1 score for each class\n",
        "report = classification_report(test_data['class'], predictions)\n",
        "print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NGRAMS"
      ],
      "metadata": {
        "id": "kDpRf5SfNJ3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv('/content/labeled_data.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "train_data = data.sample(frac=0.8, random_state=42)\n",
        "test_data = data.drop(train_data.index)\n",
        "\n",
        "# Create a CountVectorizer to convert the text data into character n-grams\n",
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 6), stop_words='english')\n",
        "train_features = vectorizer.fit_transform(train_data['tweet'])\n",
        "test_features = vectorizer.transform(test_data['tweet'])\n",
        "\n",
        "# Train a logistic regression model on the training data\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(train_features, train_data['class'])\n",
        "\n",
        "# Test the model on the testing data\n",
        "predictions = model.predict(test_features)\n",
        "\n",
        "# Calculate precision, recall, and F1 score for each class\n",
        "report = classification_report(test_data['class'], predictions)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JptrtmG_4ieU",
        "outputId": "374dbf69-a591-46f2-d8dd-bab5b3277460"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/feature_extraction/text.py:550: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.43      0.23      0.30       258\n",
            "           1       0.94      0.96      0.95      3879\n",
            "           2       0.86      0.89      0.87       820\n",
            "\n",
            "    accuracy                           0.91      4957\n",
            "   macro avg       0.74      0.69      0.71      4957\n",
            "weighted avg       0.90      0.91      0.90      4957\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF + SVM CLASSIFICATION"
      ],
      "metadata": {
        "id": "HDxtOYZjNdE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define the function to clean the text\n",
        "def clean_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in text.split() if word.lower() not in stop_words]\n",
        "    # Join the words back into a string and return\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/labeled_data.csv', index_col=0)\n",
        "\n",
        "# Clean the text data\n",
        "df['clean_text'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data, train_labels, test_labels = train_test_split(df['clean_text'], df['class'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the training data\n",
        "train_features = tfidf.fit_transform(train_data)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "test_features = tfidf.transform(test_data)\n",
        "\n",
        "# Create an SVM classifier\n",
        "svm = SVC(kernel='linear')\n",
        "\n",
        "# Train the SVM classifier\n",
        "svm.fit(train_features, train_labels)\n",
        "\n",
        "# Test the SVM classifier\n",
        "predictions = svm.predict(test_features)\n",
        "\n",
        "# Evaluate the performance of the classifier\n",
        "print(classification_report(test_labels, predictions))\n",
        "print(confusion_matrix(test_labels, predictions))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hata_I7fkukM",
        "outputId": "34b2aa73-be33-460a-d155-79c67abfc85f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.20      0.30       290\n",
            "           1       0.93      0.96      0.94      3832\n",
            "           2       0.84      0.89      0.86       835\n",
            "\n",
            "    accuracy                           0.90      4957\n",
            "   macro avg       0.78      0.68      0.70      4957\n",
            "weighted avg       0.89      0.90      0.89      4957\n",
            "\n",
            "[[  58  204   28]\n",
            " [  39 3680  113]\n",
            " [   5   90  740]]\n"
          ]
        }
      ]
    }
  ]
}